# DeepSeek温度
| 场景 | 温度 |
|------|------|
| 代码生成/数学解题 | 0.0 |
| 数据抽取/分析 | 1.0 |
| 通用对话 | 1.3 |
| 翻译 | 1.3 |
| 创意类写作/诗歌创作 | 1.5 |


# TODO
1. [ ] 任务背景确定：社会博弈？社会竞合？
2. [ ] 强化学习的建模：代码
3. [ ] 狼人杀游戏规则的提示词

# Git操作的不同作用

1. 提交 (Commit)
- 仅将修改保存到本地仓库
- 不会将更改上传到GitHub远程仓库
- 相当于git commit命令

2. 提交(修改) (Commit Amend)
- 将新的更改添加到上一次提交中
- 修改最近一次提交的内容和信息
- 相当于git commit --amend命令

3. 提交和推送 (Commit & Push)
- 先将修改提交到本地仓库
- 然后立即将更改推送到GitHub远程仓库
- 相当于git commit followed by git push

4. 提交和同步 (Commit & Sync)
- 提交本地更改
- 推送本地更改到远程
- 同时会拉取远程仓库的最新更改
- 相当于git commit + git pull + git push的组合

5. 提交和创建拉取请求 (Commit & Create Pull Request)
- 提交更改到本地
- 推送到远程仓库
- 自动创建一个新的Pull Request
- 适用于需要代码审查的协作开发流程

建议：
- 如果只是想保存本地更改，使用"提交"
- 如果要更新到GitHub，使用"提交和推送"
- 如果要确保与团队代码同步，使用"提交和同步"
- 如果需要代码审查，使用"提交和创建拉取请求"


# 强化学习中的"数据"理解

在强化学习中，**不需要预先准备标注数据集**，这是它与监督学习的本质区别。

## 强化学习数据的特点

强化学习中的"数据"是通过**智能体与环境交互动态生成的**，主要包括：

1. **状态** (State) - 游戏的当前情况
2. **动作** (Action) - 智能体做出的决策
3. **奖励** (Reward) - 环境给予的反馈信号
4. **下一状态** (Next State) - 执行动作后的新状态
5. **终止信号** (Done) - 游戏是否结束

## 你项目中的数据生成过程

对于狼人杀项目：

1. **智能体初始随机决策**：刚开始时可能使用随机策略选择动作
2. **环境反馈**：游戏环境会返回动作的结果和奖励
3. **经验收集**：这些(状态,动作,奖励,下一状态)的元组被称为"经验"
4. **策略更新**：智能体使用这些经验来更新自己的决策策略(Q表)

## 你需要准备的不是数据，而是：

1. **奖励函数**：定义好什么行为值得奖励或惩罚
   - 例如：击杀对方+2分，自己存活+0.5分，胜利+10分

2. **状态表示**：如何将复杂的游戏状态转化为智能体可理解的形式
   - 例如：存活玩家列表、已知角色信息、当前游戏阶段等

3. **训练循环**：让智能体在大量游戏中积累经验并学习

## 总结

对于你的强化学习项目，不需要预先收集数据集。你已经搭建了游戏环境，定义了数据收集机制(通过`WerewolfTrainer`类)，现在需要实现:

1. 完整的奖励系统
2. 状态表示方式
3. 学习智能体(Q-learning)
4. 训练循环

这些组件共同工作，会在训练过程中自动生成和使用数据，从而使智能体学会如何玩狼人杀游戏。